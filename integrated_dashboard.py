"""
Driver Safety Monitor Pro - Complete System
4 Models: EfficientNet + 3 Behavior Models (CNN, VGG, ResNet) + Drowsiness
Full Support: Images + Videos + Live Stream
"""

import streamlit as st
import cv2
import numpy as np
from PIL import Image
import torch
from torchvision import transforms
import torch.nn as nn
from torchvision import models
import tempfile
import os

# ====================== CONFIG ======================
st.set_page_config(
    page_title="Driver Safety Monitor Pro",
    page_icon="ğŸš—",
    layout="wide"
)

# CSS Ø§Ø­ØªØ±Ø§ÙÙŠ
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 25px;
        border-radius: 15px;
        color: white;
        text-align: center;
        margin-bottom: 30px;
        box-shadow: 0 4px 15px rgba(0,0,0,0.2);
    }
    .alert-danger {
        background: linear-gradient(135deg, #ff4444 0%, #cc0000 100%);
        color: white;
        padding: 20px;
        border-radius: 10px;
        font-weight: bold;
        text-align: center;
        animation: pulse 1.5s infinite;
        box-shadow: 0 4px 15px rgba(255,0,0,0.3);
    }
    .alert-warning {
        background: linear-gradient(135deg, #ffbb33 0%, #ff8800 100%);
        color: white;
        padding: 20px;
        border-radius: 10px;
        font-weight: bold;
        text-align: center;
        box-shadow: 0 4px 15px rgba(255,136,0,0.3);
    }
    .alert-success {
        background: linear-gradient(135deg, #00C851 0%, #007E33 100%);
        color: white;
        padding: 20px;
        border-radius: 10px;
        font-weight: bold;
        text-align: center;
        box-shadow: 0 4px 15px rgba(0,200,81,0.3);
    }
    @keyframes pulse {
        0%, 100% { opacity: 1; transform: scale(1); }
        50% { opacity: 0.8; transform: scale(1.02); }
    }
    .metric-card {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        padding: 20px;
        border-radius: 12px;
        border-left: 5px solid #667eea;
        margin: 10px 0;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
    .stButton>button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        font-weight: bold;
        border: none;
        padding: 12px 24px;
        border-radius: 8px;
        transition: all 0.3s;
    }
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 15px rgba(102,126,234,0.4);
    }
    .video-container {
        border: 3px solid #667eea;
        border-radius: 10px;
        padding: 10px;
        background: #f8f9fa;
    }
</style>
""", unsafe_allow_html=True)

# ====================== MODELS CONFIG ======================
MODELS_CONFIG = {
    "effnet": {
        "filename": "effnet.pth",
        "gdrive_id": "1GvL1w3UmOeMRISBWdKGGeeKNR2oH0MZM",
        "description": "EfficientNet Distraction Model"
    },
    "behavior_cnn": {
        "filename": "behavior_cnn.pth",
        "gdrive_id": "YOUR_CNN_MODEL_ID",  # Ø¶Ø¹ Ø§Ù„Ù€ ID Ù‡Ù†Ø§
        "description": "Behavior Detection CNN"
    },
    "behavior_vgg": {
        "filename": "behavior_vgg.pth",
        "gdrive_id": "YOUR_VGG_MODEL_ID",  # Ø¶Ø¹ Ø§Ù„Ù€ ID Ù‡Ù†Ø§
        "description": "Behavior Detection VGG16"
    },
    "behavior_resnet": {
        "filename": "behavior_resnet.pth",
        "gdrive_id": "YOUR_RESNET_MODEL_ID",  # Ø¶Ø¹ Ø§Ù„Ù€ ID Ù‡Ù†Ø§
        "description": "Behavior Detection ResNet"
    }
}

# ====================== DOWNLOAD MODELS ======================
@st.cache_resource
def download_model(filename, gdrive_id, description):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ù† Google Drive"""
    if os.path.exists(filename):
        return filename
    
    if gdrive_id.startswith("YOUR_"):
        st.warning(f"âš ï¸ {description}: ID not configured yet")
        return None
    
    try:
        import gdown
        url = f"https://drive.google.com/uc?id={gdrive_id}"
        with st.spinner(f"â³ Downloading {description}..."):
            gdown.download(url, filename, quiet=False)
        st.success(f"âœ… {description} downloaded!")
        return filename
    except Exception as e:
        st.warning(f"âš ï¸ Could not download {description}: {e}")
        return None

# ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª
model_paths = {}
with st.spinner("ğŸ”„ Loading models..."):
    for key, config in MODELS_CONFIG.items():
        model_paths[key] = download_model(
            config["filename"],
            config["gdrive_id"],
            config["description"]
        )

# ====================== MODEL ARCHITECTURES ======================
class EfficientNet_B0(nn.Module):
    """EfficientNet Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ØªØ´ØªÙŠØª"""
    def __init__(self, num_classes=10): 
        super().__init__()
        self.net = models.efficientnet_b0(weights=None)
        self.net.classifier = nn.Linear(1280, num_classes)
    
    def forward(self, x): 
        return self.net(x)

class BehaviorCNN(nn.Module):
    """CNN Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø³Ù„ÙˆÙƒÙŠØ§Øª Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©"""
    def __init__(self, num_classes=5):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(256, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

class BehaviorVGG16(nn.Module):
    """VGG16 Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø³Ù„ÙˆÙƒÙŠØ§Øª Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©"""
    def __init__(self, num_classes=5):
        super().__init__()
        self.vgg = models.vgg16(weights=None)
        self.vgg.classifier[6] = nn.Linear(4096, num_classes)
    
    def forward(self, x):
        return self.vgg(x)

class BehaviorResNet(nn.Module):
    """ResNet Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø³Ù„ÙˆÙƒÙŠØ§Øª Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©"""
    def __init__(self, num_classes=5):
        super().__init__()
        self.resnet = models.resnet50(weights=None)
        self.resnet.fc = nn.Linear(2048, num_classes)
    
    def forward(self, x):
        return self.resnet(x)

# ====================== LOAD MODELS ======================
@st.cache_resource
def load_all_models():
    """ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ø£Ø±Ø¨Ø¹Ø©"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    loaded_models = {}
    
    # 1. EfficientNet for Distraction
    if model_paths.get("effnet") and os.path.exists(model_paths["effnet"]):
        try:
            model = EfficientNet_B0(num_classes=10).to(device)
            state = torch.load(model_paths["effnet"], map_location=device, weights_only=False)
            state_dict = state.get("model", state)
            
            fixed_state = {}
            for k, v in state_dict.items():
                new_k = k.replace("net.", "").replace("module.", "")
                fixed_state[new_k] = v
            
            model.load_state_dict(fixed_state, strict=False)
            model.eval()
            loaded_models["effnet"] = model
            st.success("âœ… EfficientNet Model Loaded!")
        except Exception as e:
            st.error(f"âŒ EfficientNet Error: {e}")
    
    # 2. Behavior CNN
    if model_paths.get("behavior_cnn") and os.path.exists(model_paths["behavior_cnn"]):
        try:
            model = BehaviorCNN(num_classes=5).to(device)
            state = torch.load(model_paths["behavior_cnn"], map_location=device, weights_only=False)
            model.load_state_dict(state, strict=False)
            model.eval()
            loaded_models["behavior_cnn"] = model
            st.success("âœ… Behavior CNN Model Loaded!")
        except Exception as e:
            st.error(f"âŒ Behavior CNN Error: {e}")
    
    # 3. Behavior VGG16
    if model_paths.get("behavior_vgg") and os.path.exists(model_paths["behavior_vgg"]):
        try:
            model = BehaviorVGG16(num_classes=5).to(device)
            state = torch.load(model_paths["behavior_vgg"], map_location=device, weights_only=False)
            model.load_state_dict(state, strict=False)
            model.eval()
            loaded_models["behavior_vgg"] = model
            st.success("âœ… Behavior VGG16 Model Loaded!")
        except Exception as e:
            st.error(f"âŒ Behavior VGG Error: {e}")
    
    # 4. Behavior ResNet
    if model_paths.get("behavior_resnet") and os.path.exists(model_paths["behavior_resnet"]):
        try:
            model = BehaviorResNet(num_classes=5).to(device)
            state = torch.load(model_paths["behavior_resnet"], map_location=device, weights_only=False)
            model.load_state_dict(state, strict=False)
            model.eval()
            loaded_models["behavior_resnet"] = model
            st.success("âœ… Behavior ResNet Model Loaded!")
        except Exception as e:
            st.error(f"âŒ Behavior ResNet Error: {e}")
    
    return loaded_models, device

models_dict, device = load_all_models()

# ====================== LABELS ======================
DISTRACTION_LABELS = {
    0: "Safe Driving", 1: "Texting Right", 2: "Talking Right",
    3: "Texting Left", 4: "Talking Left", 5: "Operating Radio",
    6: "Drinking", 7: "Reaching Behind", 8: "Hair/Makeup", 9: "Talking to Passenger"
}

BEHAVIOR_LABELS = {
    0: "Normal Driving",
    1: "Aggressive Driving", 
    2: "Distracted Driving",
    3: "Drowsy Driving",
    4: "Drunk Driving"
}

# Transforms
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# ====================== DETECTION FUNCTIONS ======================
def detect_distraction_effnet(frame, model, device):
    """ÙƒØ´Ù Ø§Ù„ØªØ´ØªÙŠØª - EfficientNet"""
    if model is None:
        return frame, False, "Model Not Loaded", 0.0
    
    try:
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil = Image.fromarray(rgb)
        tensor = transform(pil).unsqueeze(0).to(device)
        
        with torch.no_grad():
            output = model(tensor)
            probs = torch.softmax(output, dim=1)[0]
            idx = torch.argmax(probs).item()
            conf = probs[idx].item()
        
        label = DISTRACTION_LABELS.get(idx, "Unknown")
        color = (0, 255, 0) if idx == 0 else (0, 0, 255)
        
        cv2.putText(frame, f"EfficientNet: {label}", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        cv2.putText(frame, f"Conf: {conf:.1%}", (10, 60), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        
        return frame, idx != 0, label, conf
    except Exception as e:
        return frame, False, f"Error: {e}", 0.0

def detect_behavior_model(frame, model, device, model_name):
    """ÙƒØ´Ù Ø§Ù„Ø³Ù„ÙˆÙƒ - CNN/VGG/ResNet"""
    if model is None:
        return frame, False, "Model Not Loaded", 0.0
    
    try:
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil = Image.fromarray(rgb)
        tensor = transform(pil).unsqueeze(0).to(device)
        
        with torch.no_grad():
            output = model(tensor)
            probs = torch.softmax(output, dim=1)[0]
            idx = torch.argmax(probs).item()
            conf = probs[idx].item()
        
        label = BEHAVIOR_LABELS.get(idx, "Unknown")
        color = (0, 255, 0) if idx == 0 else (0, 0, 255)
        
        y_pos = 90 if "CNN" in model_name else (120 if "VGG" in model_name else 150)
        cv2.putText(frame, f"{model_name}: {label}", (10, y_pos), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        cv2.putText(frame, f"{conf:.1%}", (10, y_pos + 20), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
        
        return frame, idx != 0, label, conf
    except Exception as e:
        return frame, False, f"Error", 0.0

def detect_drowsiness(frame):
    """ÙƒØ´Ù Ø§Ù„Ù†Ø¹Ø§Ø³ - OpenCV Cascade"""
    try:
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        eye_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_eye.xml'
        )
        
        faces = face_cascade.detectMultiScale(gray, 1.1, 4, minSize=(80, 80))
        closed_eyes = 0
        total_eyes = 0
        
        for (x, y, w, h) in faces:
            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 200, 0), 2)
            roi_gray = gray[y:y+h, x:x+w]
            eyes = eye_cascade.detectMultiScale(roi_gray, 1.05, 3, minSize=(20, 20))
            total_eyes += len(eyes)
            
            for (ex, ey, ew, eh) in eyes:
                eye_region = roi_gray[ey:ey+eh, ex:ex+ew]
                
                if eye_region.size > 0:
                    variance = np.var(eye_region)
                    if variance < 50:
                        closed_eyes += 1
                        cv2.rectangle(frame, (x+ex, y+ey), (x+ex+ew, y+ey+eh), (0, 0, 255), 2)
                        cv2.putText(frame, "CLOSED", (x+ex, y+ey-5), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1)
                    else:
                        cv2.rectangle(frame, (x+ex, y+ey), (x+ex+ew, y+ey+eh), (0, 255, 0), 2)
                        cv2.putText(frame, "OPEN", (x+ex, y+ey-5), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)
        
        is_drowsy = (closed_eyes >= 2) and (total_eyes > 0)
        confidence = (closed_eyes / max(total_eyes, 1)) if total_eyes > 0 else 0.0
        
        status = "âš ï¸ DROWSY!" if is_drowsy else "âœ“ ALERT"
        color = (0, 0, 255) if is_drowsy else (0, 255, 0)
        cv2.putText(frame, status, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
        cv2.putText(frame, f"Eyes: {total_eyes} (Closed: {closed_eyes})", (10, 60), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        return frame, is_drowsy, confidence
    except Exception as e:
        cv2.putText(frame, f"Detection Error", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        return frame, False, 0.0

def combined_analysis(drowsy_result, side_results):
    """Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¯Ù…Ø¬ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª"""
    drowsy, drowsy_conf = drowsy_result
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ø£Ø±Ø¨Ø¹Ø©
    distraction_risk = side_results.get("distraction", (False, "N/A", 0.0))
    cnn_risk = side_results.get("cnn", (False, "N/A", 0.0))
    vgg_risk = side_results.get("vgg", (False, "N/A", 0.0))
    resnet_risk = side_results.get("resnet", (False, "N/A", 0.0))
    
    risk_score = 0
    alerts = []
    
    # Ø§Ù„Ù†Ø¹Ø§Ø³ (ÙˆØ²Ù† 35%)
    if drowsy:
        risk_score += 35
        alerts.append(f"âš ï¸ Ù†Ø¹Ø§Ø³ Ù…ÙƒØªØ´Ù ({drowsy_conf:.0%})")
    
    # Ø§Ù„ØªØ´ØªÙŠØª - EfficientNet (ÙˆØ²Ù† 25%)
    if distraction_risk[0]:
        risk_score += 25
        alerts.append(f"âš ï¸ ØªØ´ØªÙŠØª: {distraction_risk[1]} ({distraction_risk[2]:.0%})")
    
    # Ø§Ù„Ø³Ù„ÙˆÙƒ - CNN (ÙˆØ²Ù† 15%)
    if cnn_risk[0]:
        risk_score += 15
        alerts.append(f"âš ï¸ CNN: {cnn_risk[1]} ({cnn_risk[2]:.0%})")
    
    # Ø§Ù„Ø³Ù„ÙˆÙƒ - VGG (ÙˆØ²Ù† 13%)
    if vgg_risk[0]:
        risk_score += 13
        alerts.append(f"âš ï¸ VGG: {vgg_risk[1]} ({vgg_risk[2]:.0%})")
    
    # Ø§Ù„Ø³Ù„ÙˆÙƒ - ResNet (ÙˆØ²Ù† 12%)
    if resnet_risk[0]:
        risk_score += 12
        alerts.append(f"âš ï¸ ResNet: {resnet_risk[1]} ({resnet_risk[2]:.0%})")
    
    # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·Ø±
    if risk_score >= 50:
        status = "ğŸ”´ Ø®Ø·Ø± Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹ - ØªÙˆÙ‚Ù ÙÙˆØ±Ø§Ù‹!"
        alert_class = "alert-danger"
    elif risk_score >= 30:
        status = "ğŸŸ¡ ØªØ­Ø°ÙŠØ± - Ø§Ù†ØªØ¨Ù‡ Ù„Ù„Ø·Ø±ÙŠÙ‚"
        alert_class = "alert-warning"
    else:
        status = "ğŸŸ¢ Ù‚ÙŠØ§Ø¯Ø© Ø¢Ù…Ù†Ø©"
        alert_class = "alert-success"
    
    return {
        "status": status,
        "risk_score": risk_score,
        "alerts": alerts,
        "alert_class": alert_class,
        "details": {
            "drowsy": drowsy,
            "distraction": distraction_risk,
            "cnn": cnn_risk,
            "vgg": vgg_risk,
            "resnet": resnet_risk
        }
    }

# ====================== VIDEO PROCESSING ======================
def process_video_file(video_path, progress_bar, status_text, result_container):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙÙŠØ¯ÙŠÙˆ ÙƒØ§Ù…Ù„"""
    cap = cv2.VideoCapture(video_path)
    
    if not cap.isOpened():
        st.error("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† ÙØªØ­ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ!")
        return []
    
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = int(cap.get(cv2.CAP_PROP_FPS)) if int(cap.get(cv2.CAP_PROP_FPS)) > 0 else 30
    
    results = []
    frame_idx = 0
    process_every = 10  # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ 10 Ø¥Ø·Ø§Ø±Ø§Øª Ù„ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        if frame_idx % process_every == 0:
            try:
                # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¥Ø·Ø§Ø±: Ù†ØµÙ Ø£Ù…Ø§Ù…ÙŠ + Ù†ØµÙ Ø¬Ø§Ù†Ø¨ÙŠ
                h, w = frame.shape[:2]
                front_frame = cv2.resize(frame[:, :w//2], (640, 480))
                side_frame = cv2.resize(frame[:, w//2:], (640, 480))
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ© (Ø§Ù„Ù†Ø¹Ø§Ø³)
                _, drowsy, drowsy_conf = detect_drowsiness(front_frame.copy())
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© (Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ø£Ø±Ø¨Ø¹Ø©)
                side_results = {}
                
                # EfficientNet
                _, dist_risk, dist_label, dist_conf = detect_distraction_effnet(
                    side_frame.copy(), models_dict.get("effnet"), device
                )
                side_results["distraction"] = (dist_risk, dist_label, dist_conf)
                
                # Behavior CNN
                _, cnn_risk, cnn_label, cnn_conf = detect_behavior_model(
                    side_frame.copy(), models_dict.get("behavior_cnn"), device, "CNN"
                )
                side_results["cnn"] = (cnn_risk, cnn_label, cnn_conf)
                
                # Behavior VGG
                _, vgg_risk, vgg_label, vgg_conf = detect_behavior_model(
                    side_frame.copy(), models_dict.get("behavior_vgg"), device, "VGG"
                )
                side_results["vgg"] = (vgg_risk, vgg_label, vgg_conf)
                
                # Behavior ResNet
                _, resnet_risk, resnet_label, resnet_conf = detect_behavior_model(
                    side_frame.copy(), models_dict.get("behavior_resnet"), device, "ResNet"
                )
                side_results["resnet"] = (resnet_risk, resnet_label, resnet_conf)
                
                # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¯Ù…Ø¬
                analysis = combined_analysis((drowsy, drowsy_conf), side_results)
                
                results.append({
                    "frame": frame_idx,
                    "time": frame_idx / fps,
                    "analysis": analysis
                })
                
                # Ø¹Ø±Ø¶ ØªØ­Ø¯ÙŠØ« Ù…Ø¨Ø§Ø´Ø±
                if frame_idx % (process_every * 10) == 0:
                    with result_container:
                        st.write(f"â±ï¸ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© {frame_idx // fps} - {analysis['status']}")
            
            except Exception as e:
                st.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¥Ø·Ø§Ø± {frame_idx}: {e}")
        
        frame_idx += 1
        progress = min(frame_idx / frame_count, 1.0)
        progress_bar.progress(progress)
        status_text.text(f"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø·Ø§Ø± {frame_idx}/{frame_count}")
    
    cap.release()
    return results

# ====================== LIVE STREAM PROCESSING ======================
def process_live_stream(front_cam_id, side_cam_id, stop_button_placeholder):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù…Ù† ÙƒØ§Ù…ÙŠØ±ØªÙŠÙ†"""
    front_cap = cv2.VideoCapture(front_cam_id)
    side_cap = cv2.VideoCapture(side_cam_id)
    
    if not front_cap.isOpened() or not side_cap.isOpened():
        st.error("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª! ØªØ£ÙƒØ¯ Ù…Ù† ØªÙˆØµÙŠÙ„Ù‡Ø§ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­")
        return
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¨Ø«
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("### ğŸ“¹ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©")
        front_placeholder = st.empty()
    
    with col2:
        st.markdown("### ğŸ“¹ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©")
        side_placeholder = st.empty()
    
    with col3:
        st.markdown("### ğŸ“Š Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±")
        analysis_placeholder = st.empty()
    
    # Ø²Ø± Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù
    stop_clicked = stop_button_placeholder.button("â¹ï¸ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø«", use_container_width=True, type="primary")
    
    frame_count = 0
    
    try:
        while not stop_clicked:
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª
            ret1, front_frame = front_cap.read()
            ret2, side_frame = side_cap.read()
            
            if not ret1 or not ret2:
                st.warning("âš ï¸ ÙÙ‚Ø¯ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§")
                break
            
            # ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù…
            front_frame = cv2.resize(front_frame, (640, 480))
            side_frame = cv2.resize(side_frame, (640, 480))
            
            # Ø§Ù„ØªØ­Ù„ÙŠÙ„ (ÙƒÙ„ 5 Ø¥Ø·Ø§Ø±Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡)
            if frame_count % 5 == 0:
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø¹Ø§Ø³
                front_annotated, drowsy, drowsy_conf = detect_drowsiness(front_frame.copy())
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©
                side_annotated = side_frame.copy()
                side_results = {}
                
                # EfficientNet
                side_annotated, dist_risk, dist_label, dist_conf = detect_distraction_effnet(
                    side_annotated, models_dict.get("effnet"), device
                )
                side_results["distraction"] = (dist_risk, dist_label, dist_conf)
                
                # Behavior Models
                side_annotated, cnn_risk, cnn_label, cnn_conf = detect_behavior_model(
                    side_annotated, models_dict.get("behavior_cnn"), device, "CNN"
                )
                side_results["cnn"] = (cnn_risk, cnn_label, cnn_conf)
                
                side_annotated, vgg_risk, vgg_label, vgg_conf = detect_behavior_model(
                    side_annotated, models_dict.get("behavior_vgg"), device, "VGG"
                )
                side_results["vgg"] = (vgg_risk, vgg_label, vgg_conf)
                
                side_annotated, resnet_risk, resnet_label, resnet_conf = detect_behavior_model(
                    side_annotated, models_dict.get("behavior_resnet"), device, "ResNet"
                )
                side_results["resnet"] = (resnet_risk, resnet_label, resnet_conf)
                
                # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¯Ù…Ø¬
                analysis = combined_analysis((drowsy, drowsy_conf), side_results)
                
                # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                front_placeholder.image(front_annotated, channels="BGR", use_container_width=True)
                side_placeholder.image(side_annotated, channels="BGR", use_container_width=True)
                
                with analysis_placeholder.container():
                    st.markdown(f'<div class="{analysis["alert_class"]}">{analysis["status"]}</div>', 
                               unsafe_allow_html=True)
                    st.metric("Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·Ø±", f"{analysis['risk_score']}%")
                    
                    if analysis['alerts']:
                        st.markdown("**âš ï¸ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª:**")
                        for alert in analysis['alerts']:
                            st.warning(alert)
            
            frame_count += 1
            
            # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø²Ø± Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù
            stop_clicked = stop_button_placeholder.button("â¹ï¸ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø«", 
                                                          use_container_width=True, 
                                                          type="primary",
                                                          key=f"stop_{frame_count}")
    
    finally:
        front_cap.release()
        side_cap.release()
        st.success("âœ… ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø« Ø¨Ù†Ø¬Ø§Ø­")

# ====================== MAIN UI ======================
st.markdown('<div class="main-header"><h1>ğŸš— Driver Safety Monitor Pro</h1><p>Ù†Ø¸Ø§Ù… Ù…ØªÙƒØ§Ù…Ù„ Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø³Ù„Ø§Ù…Ø© - 4 Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ</p></div>', unsafe_allow_html=True)

# Sidebar
with st.sidebar:
    st.header("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„ØªØ­ÙƒÙ…")
    
    input_mode = st.radio(
        "Ø§Ø®ØªØ± Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„:",
        ["ğŸ“¸ Ø±ÙØ¹ ØµÙˆØ±", "ğŸ¥ Ø±ÙØ¹ ÙÙŠØ¯ÙŠÙˆ", "ğŸ“¹ Ø¨Ø« Ù…Ø¨Ø§Ø´Ø±"],
        index=0
    )
    
    st.markdown("---")
    st.markdown("### ğŸ“Š Ø­Ø§Ù„Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª")
    
    models_status = {
        "EfficientNet": "effnet" in models_dict,
        "Behavior CNN": "behavior_cnn" in models_dict,
        "Behavior VGG16": "behavior_vgg" in models_dict,
        "Behavior ResNet": "behavior_resnet" in models_dict,
        "Drowsiness (OpenCV)": True
    }
    
    for model_name, status in models_status.items():
        icon = "âœ…" if status else "âŒ"
        st.write(f"{icon} {model_name}")
    
    total_models = sum(models_status.values())
    st.metric("Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ø¬Ø§Ù‡Ø²Ø©", f"{total_models}/5")
    
    st.markdown("---")
    st.info("ğŸ’¡ **Ù†ØµÙŠØ­Ø©:** Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¶Ø§Ø¡Ø© Ø¬ÙŠØ¯Ø© ÙˆÙˆØ¶ÙˆØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§")

# Main Content
st.markdown("---")

if input_mode == "ğŸ“¸ Ø±ÙØ¹ ØµÙˆØ±":
    st.subheader("ğŸ“¸ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±")
    
    col_upload1, col_upload2 = st.columns(2)
    
    with col_upload1:
        st.markdown("#### ğŸ“¹ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©")
        front_upload = st.file_uploader(
            "Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© Ø§Ù„Ø³Ø§Ø¦Ù‚ Ù…Ù† Ø§Ù„Ø£Ù…Ø§Ù…",
            type=['jpg', 'jpeg', 'png', 'bmp'],
            key="front_img",
            help="Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù†Ø¹Ø§Ø³"
        )
    
    with col_upload2:
        st.markdown("#### ğŸ“¹ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©")
        side_upload = st.file_uploader(
            "Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© Ø§Ù„Ø³Ø§Ø¦Ù‚ Ù…Ù† Ø§Ù„Ø¬Ø§Ù†Ø¨",
            type=['jpg', 'jpeg', 'png', 'bmp'],
            key="side_img",
            help="Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ØªØ´ØªØª ÙˆØ§Ù„Ø³Ù„ÙˆÙƒ"
        )
    
    if st.button("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¢Ù†", use_container_width=True, type="primary"):
        if front_upload and side_upload:
            with st.spinner("â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… 5 Ù…ÙˆØ¯ÙŠÙ„Ø§Øª..."):
                try:
                    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±
                    front_bytes = np.asarray(bytearray(front_upload.read()), dtype=np.uint8)
                    front_frame = cv2.imdecode(front_bytes, cv2.IMREAD_COLOR)
                    front_frame = cv2.resize(front_frame, (640, 480))
                    
                    side_bytes = np.asarray(bytearray(side_upload.read()), dtype=np.uint8)
                    side_frame = cv2.imdecode(side_bytes, cv2.IMREAD_COLOR)
                    side_frame = cv2.resize(side_frame, (640, 480))
                    
                    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
                    front_annotated, drowsy, drowsy_conf = detect_drowsiness(front_frame.copy())
                    
                    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© (4 Ù…ÙˆØ¯ÙŠÙ„Ø§Øª)
                    side_annotated = side_frame.copy()
                    side_results = {}
                    
                    # 1. EfficientNet
                    side_annotated, dist_risk, dist_label, dist_conf = detect_distraction_effnet(
                        side_annotated, models_dict.get("effnet"), device
                    )
                    side_results["distraction"] = (dist_risk, dist_label, dist_conf)
                    
                    # 2. Behavior CNN
                    side_annotated, cnn_risk, cnn_label, cnn_conf = detect_behavior_model(
                        side_annotated, models_dict.get("behavior_cnn"), device, "CNN"
                    )
                    side_results["cnn"] = (cnn_risk, cnn_label, cnn_conf)
                    
                    # 3. Behavior VGG
                    side_annotated, vgg_risk, vgg_label, vgg_conf = detect_behavior_model(
                        side_annotated, models_dict.get("behavior_vgg"), device, "VGG"
                    )
                    side_results["vgg"] = (vgg_risk, vgg_label, vgg_conf)
                    
                    # 4. Behavior ResNet
                    side_annotated, resnet_risk, resnet_label, resnet_conf = detect_behavior_model(
                        side_annotated, models_dict.get("behavior_resnet"), device, "ResNet"
                    )
                    side_results["resnet"] = (resnet_risk, resnet_label, resnet_conf)
                    
                    # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¯Ù…Ø¬
                    analysis = combined_analysis((drowsy, drowsy_conf), side_results)
                    
                    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                    st.markdown("---")
                    st.markdown(f'<div class="{analysis["alert_class"]}">{analysis["status"]}</div>', 
                               unsafe_allow_html=True)
                    
                    col1, col2, col3 = st.columns([1, 1, 1])
                    
                    with col1:
                        st.markdown('<div class="video-container">', unsafe_allow_html=True)
                        st.image(front_annotated, channels="BGR", caption="ğŸ“¹ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©", 
                                use_container_width=True)
                        st.markdown('</div>', unsafe_allow_html=True)
                    
                    with col2:
                        st.markdown('<div class="video-container">', unsafe_allow_html=True)
                        st.image(side_annotated, channels="BGR", caption="ğŸ“¹ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©", 
                                use_container_width=True)
                        st.markdown('</div>', unsafe_allow_html=True)
                    
                    with col3:
                        st.markdown("### ğŸ“Š Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ")
                        st.metric("Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·Ø± Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ", f"{analysis['risk_score']}%",
                                 delta=f"{analysis['risk_score']-50}%" if analysis['risk_score'] > 50 else None,
                                 delta_color="inverse")
                        
                        st.markdown("---")
                        
                        if analysis['alerts']:
                            st.markdown("**âš ï¸ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©:**")
                            for i, alert in enumerate(analysis['alerts'], 1):
                                st.warning(f"{i}. {alert}")
                        else:
                            st.success("âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø®Ø§Ø·Ø± - Ù‚ÙŠØ§Ø¯Ø© Ø¢Ù…Ù†Ø©!")
                        
                        st.markdown("---")
                        st.markdown("**ğŸ“‹ ØªÙØ§ØµÙŠÙ„ ÙƒÙ„ Ù…ÙˆØ¯ÙŠÙ„:**")
                        
                        details = analysis['details']
                        st.write(f"ğŸ”¹ Ù†Ø¹Ø§Ø³: {'Ù†Ø¹Ù… âš ï¸' if details['drowsy'] else 'Ù„Ø§ âœ“'}")
                        st.write(f"ğŸ”¹ ØªØ´ØªÙŠØª: {details['distraction'][1]} ({details['distraction'][2]:.0%})")
                        st.write(f"ğŸ”¹ CNN: {details['cnn'][1]} ({details['cnn'][2]:.0%})")
                        st.write(f"ğŸ”¹ VGG: {details['vgg'][1]} ({details['vgg'][2]:.0%})")
                        st.write(f"ğŸ”¹ ResNet: {details['resnet'][1]} ({details['resnet'][2]:.0%})")
                
                except Exception as e:
                    st.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")
        else:
            st.error("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±ØªÙŠÙ† Ø£ÙˆÙ„Ø§Ù‹!")

elif input_mode == "ğŸ¥ Ø±ÙØ¹ ÙÙŠØ¯ÙŠÙˆ":
    st.subheader("ğŸ¥ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ")
    
    st.info("ğŸ“Œ **Ù…Ù„Ø§Ø­Ø¸Ø©:** ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¹Ù„Ù‰ ÙƒØ§Ù…ÙŠØ±ØªÙŠÙ† Ø¬Ù†Ø¨Ø§Ù‹ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ (Ø£Ù…Ø§Ù…ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø±ØŒ Ø¬Ø§Ù†Ø¨ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ÙŠÙ…ÙŠÙ†)")
    
    video_upload = st.file_uploader(
        "Ø§Ø±ÙØ¹ ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©",
        type=['mp4', 'avi', 'mov', 'mkv', 'webm'],
        key="video",
        help="Ø­Ø¬Ù… Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø£Ù‚Ù„ Ù…Ù† 200 Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª"
    )
    
    if video_upload:
        st.video(video_upload)
        
        if st.button("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø¢Ù†", use_container_width=True, type="primary"):
            with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
                tmp_file.write(video_upload.read())
                video_path = tmp_file.name
            
            st.markdown("---")
            st.subheader("â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...")
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            result_container = st.container()
            
            try:
                results = process_video_file(video_path, progress_bar, status_text, result_container)
                
                st.success("âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
                
                # Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                st.markdown("---")
                st.markdown("### ğŸ“Š Ù…Ù„Ø®Øµ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ")
                
                if results:
                    total_frames = len(results)
                    high_risk_frames = sum(1 for r in results if r['analysis']['risk_score'] >= 50)
                    medium_risk_frames = sum(1 for r in results if 30 <= r['analysis']['risk_score'] < 50)
                    safe_frames = sum(1 for r in results if r['analysis']['risk_score'] < 30)
                    
                    col1, col2, col3, col4 = st.columns(4)
                    col1.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª", total_frames)
                    col2.metric("Ø®Ø·Ø± Ø¹Ø§Ù„ÙŠ ğŸ”´", high_risk_frames)
                    col3.metric("ØªØ­Ø°ÙŠØ± ğŸŸ¡", medium_risk_frames)
                    col4.metric("Ø¢Ù…Ù† ğŸŸ¢", safe_frames)
                    
                    safety_percentage = (safe_frames / total_frames * 100) if total_frames > 0 else 0
                    st.progress(safety_percentage / 100)
                    st.write(f"**Ù†Ø³Ø¨Ø© Ø§Ù„Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {safety_percentage:.1f}%**")
                    
                    # Ø¹Ø±Ø¶ Ø§Ù„ØªÙØ§ØµÙŠÙ„
                    if st.checkbox("ğŸ“‹ Ø¹Ø±Ø¶ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„Ø©"):
                        st.markdown("### ğŸ“ Ø³Ø¬Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„")
                        for result in results:
                            time_str = f"{int(result['time']//60):02d}:{int(result['time']%60):02d}"
                            with st.expander(f"â±ï¸ Ø§Ù„ÙˆÙ‚Øª {time_str} - {result['analysis']['status']}"):
                                st.write(f"**Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·Ø±:** {result['analysis']['risk_score']}%")
                                if result['analysis']['alerts']:
                                    st.write("**Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª:**")
                                    for alert in result['analysis']['alerts']:
                                        st.write(f"- {alert}")
                else:
                    st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬")
            
            except Exception as e:
                st.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {e}")
            
            finally:
                os.unlink(video_path)

else:  # Ø¨Ø« Ù…Ø¨Ø§Ø´Ø±
    st.subheader("ğŸ“¹ Ø§Ù„Ø¨Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù…Ù† Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª")
    
    st.warning("âš ï¸ **ØªÙ†Ø¨ÙŠÙ‡:** Ø§Ù„Ø¨Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± ÙŠØ¹Ù…Ù„ ÙÙ‚Ø· Ø¹Ù†Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø­Ù„ÙŠ. Ù„Ø§ ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ Streamlit Cloud!")
    
    col1, col2 = st.columns(2)
    
    with col1:
        front_cam_id = st.number_input(
            "Ø±Ù‚Ù… Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©",
            value=0,
            min_value=0,
            max_value=10,
            help="Ø¹Ø§Ø¯Ø© 0 Ø£Ùˆ 1"
        )
    
    with col2:
        side_cam_id = st.number_input(
            "Ø±Ù‚Ù… Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©",
            value=1,
            min_value=0,
            max_value=10,
            help="Ø¹Ø§Ø¯Ø© 1 Ø£Ùˆ 2"
        )
    
    st.markdown("---")
    
    stop_button_placeholder = st.empty()
    
    if st.button("â–¶ï¸ Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø±", use_container_width=True, type="primary"):
        st.markdown("---")
        try:
            process_live_stream(int(front_cam_id), int(side_cam_id), stop_button_placeholder)
        except Exception as e:
            st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø«: {e}")
            st.info("ğŸ’¡ ØªØ£ÙƒØ¯ Ù…Ù† ØªÙˆØµÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù…Ø­Ù„ÙŠØ§Ù‹")

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; padding: 20px; background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%); border-radius: 10px;'>
    <h3>ğŸš— Driver Safety Monitor Pro v2.0</h3>
    <p><strong>Powered by:</strong> PyTorch â€¢ OpenCV â€¢ Streamlit</p>
    <p><strong>Models:</strong> EfficientNet B0 | Custom CNN | VGG16 | ResNet50</p>
    <p style='color: #667eea;'><strong>Developed with â¤ï¸ for Safer Roads</strong></p>
</div>
""", unsafe_allow_html=True)
            "
